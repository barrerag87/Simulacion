{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging y Boosting son dos técnicas de ensamblaje en aprendizaje automático que tienen como objetivo mejorar la precisión y la estabilidad de los modelos predictivos. A continuación, se explica cada una de estas técnicas:\n",
    "\n",
    "### Bagging\n",
    "**Bagging**, abreviatura de *Bootstrap Aggregating*, es un método de ensamblaje que busca mejorar la estabilidad y la precisión de los algoritmos de aprendizaje automático mediante la creación de múltiples versiones de un predictor y luego combinándolas para formar un modelo final. La técnica se aplica generalmente a métodos de alta varianza y baja sesgo, como árboles de decisión.\n",
    "\n",
    "**Pasos clave**:\n",
    "1. **Bootstrap**: Se generan múltiples subconjuntos de datos de entrenamiento mediante muestreo con reemplazo del conjunto original.\n",
    "2. **Entrenamiento independiente**: Cada subconjunto de datos se usa para entrenar un modelo independiente.\n",
    "3. **Agregación**: Los modelos individuales se combinan promediando sus predicciones (en el caso de regresión) o por votación mayoritaria (en el caso de clasificación).\n",
    "\n",
    "**Ventajas**:\n",
    "- Reduce la varianza, evitando el sobreajuste.\n",
    "- Mejora la robustez del modelo al promediar múltiples estimaciones.\n",
    "\n",
    "### Boosting\n",
    "**Boosting** es un método de ensamblaje secuencial que combina múltiples modelos débiles para formar un modelo fuerte. A diferencia del Bagging, en el Boosting cada modelo nuevo se construye focalizándose en los errores cometidos por los modelos anteriores.\n",
    "\n",
    "**Pasos**:\n",
    "1. **Entrenar el modelo inicial**: Se entrena un modelo en el conjunto de datos y se evalúan los errores.\n",
    "2. **Enfocar en errores**: Se asigna más peso a las instancias mal clasificadas por el modelo anterior.\n",
    "3. **Iterar**: Se repite el proceso, cada nuevo modelo se concentra en los errores más significativos de los modelos anteriores.\n",
    "4. **Combinación ponderada**: Los modelos se combinan usando una suma ponderada para obtener la predicción final.\n",
    "\n",
    "**Ventajas**:\n",
    "- Mejora la precisión al reducir tanto el sesgo como la varianza.\n",
    "- Eficaz para reducir errores en problemas complejos.\n",
    "\n",
    "Ambas técnicas utilizan la idea de \"sabiduría de la multitud\" para obtener mejores predicciones que cualquier modelo individual podría proporcionar. Sin embargo, Boosting puede ser más propenso al sobreajuste si los datos son muy ruidosos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claro, vamos a profundizar en los detalles matemáticos de Bagging y Boosting para ofrecer una mejor comprensión de cómo funcionan estos métodos de ensamblaje.\n",
    "\n",
    "### Bagging (Bootstrap Aggregating)\n",
    "\n",
    "**Fundamento matemático**:\n",
    "El Bagging se basa en el muestreo aleatorio con reemplazo, conocido como *bootstrap*. Si tienes un conjunto de datos $ D $ con $ N $ muestras, el muestreo bootstrap crea subconjuntos $ D_i $ seleccionando $ N $ muestras de $ D $ al azar con reemplazo. Cada $ D_i $ es probable que tenga algunas muestras repetidas y otras no presentes.\n",
    "\n",
    "Cada uno de estos subconjuntos se utiliza para entrenar modelos independientes $ M_i $, generalmente del mismo tipo pero entrenados en diferentes \"vistas\" de los datos. El resultado final de Bagging es un modelo ensamblado que promedia las predicciones de todos estos modelos $ M_i $. Matemáticamente, la predicción final $ \\hat{y} $ para una instancia $ x $ se calcula como:\n",
    "\n",
    "$\n",
    "\\hat{y}(x) = \\frac{1}{B} \\sum_{i=1}^{B} M_i(x)\n",
    "$\n",
    "\n",
    "donde $ B $ es el número de modelos en el ensamblaje.\n",
    "\n",
    "**Reducción de varianza**:\n",
    "El método de Bagging reduce la varianza porque al promediar múltiples estimaciones que tienen errores no correlacionados, el error conjunto tiende a cancelarse. Esto se debe al efecto de promediar una serie de observaciones independientes.\n",
    "\n",
    "### Boosting\n",
    "\n",
    "**Fundamento matemático**:\n",
    "Boosting construye modelos de manera iterativa enfocándose en errores de predicciones anteriores. Suponiendo que comenzamos con un conjunto de datos $ D $ y un modelo inicial $ M_1 $, Boosting ajusta sucesivos modelos $ M_2, M_3, \\ldots, M_B $ donde cada modelo intenta corregir los errores del modelo anterior.\n",
    "\n",
    "Cada modelo $ M_i $ se entrena usando un conjunto de pesos $ w_n $ para cada muestra en el conjunto de datos, donde inicialmente todos los pesos pueden ser iguales. Después de cada modelo, los pesos se ajustan para aumentar la importancia de las muestras que fueron mal clasificadas:\n",
    "\n",
    "$\n",
    "w_n \\leftarrow w_n \\cdot \\exp(\\alpha_i \\cdot I(y_n \\neq M_i(x_n)))\n",
    "$\n",
    "\n",
    "donde $ \\alpha_i $ es la tasa de aprendizaje y $ I $ es la función indicadora, que es 1 si $ y_n \\neq M_i(x_n) $ y 0 de lo contrario.\n",
    "\n",
    "El modelo final $ M $ es una combinación lineal de todos los modelos ponderados por su precisión respectiva:\n",
    "\n",
    "$\n",
    "M(x) = \\sum_{i=1}^{B} \\alpha_i M_i(x)\n",
    "$\n",
    "\n",
    "**Reducción de sesgo y varianza**:\n",
    "A diferencia de Bagging, Boosting puede reducir tanto el sesgo como la varianza. Al concentrarse en los errores de los modelos anteriores, Boosting puede construir un modelo final que sea más preciso y generalizable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bagging Classifier: 0.9814814814814815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rodri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Cargar datos\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instanciar el modelo de Bagging\n",
    "bagging_clf = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of Bagging Classifier:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of AdaBoost Classifier: 0.9259259259259259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rodri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Cargar datos\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instanciar el modelo AdaBoost\n",
    "adaboost_clf = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo\n",
    "y_pred = adaboost_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of AdaBoost Classifier:\", accuracy)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
